This code performs a multi-stage analysis and predictive modeling on earthquake data, 
starting from data preprocessing, geographic visualization, and concluding with machine learning for predictive analysis. 
It integrates several libraries such as Pandas, NumPy, Basemap, Matplotlib, and Keras to accomplish these tasks in a structured manner.
Initially, earthquake data from a CSV file is loaded, and specific columns (Date, Time, Latitude, Longitude, Depth, Magnitude) are selected for analysis. 
The code preprocesses the data by combining the date and time fields into a single timestamp.
This is achieved through string manipulation and converting the combined date-time values into UNIX timestamps. 
Erroneous values are handled using a try-except block to avoid data corruption, and rows with invalid timestamps are excluded.
For visualization, the Basemap library is employed to create a world map projection that displays the locations of earthquakes based on their latitude and longitude. 
This step provides an overview of all affected regions globally, illustrating how the earthquake incidents are distributed geographically. 
Markers are plotted on the map to indicate earthquake occurrences.
Following the visualization, the code prepares data for machine learning by defining features (Timestamp, Latitude, Longitude) and target variables (Magnitude, Depth). 
A train-test split is performed with an 80-20 ratio to separate the data into training and test sets. 
The model aims to predict both earthquake magnitude and depth based on temporal and geographic information.
A neural network model is constructed using Keras with the Sequential API. 
The model consists of dense layers with adjustable hyperparameters such as the number of neurons, activation functions, and optimization methods. 
The output layer uses a sigmoid activation function to perform binary classification, predicting earthquake characteristics. 
A grid search is employed through GridSearchCV to identify the optimal model architecture, tuning parameters like neuron count, batch size, and activation function. 
This helps ensure the model is well-optimized for the given data.The final optimized model is evaluated on the test dataset, providing metrics such as loss and accuracy to assess performance. 
The code also prints the best hyperparameters identified during the grid search, offering insights into the most effective model configuration.
